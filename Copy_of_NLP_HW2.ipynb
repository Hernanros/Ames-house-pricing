{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of NLP_HW2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hernanros/Ames-house-pricing/blob/master/Copy_of_NLP_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb1yXLWqNO3s",
        "colab_type": "text"
      },
      "source": [
        "##  Text generation by Markov chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf77BtrtNO3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIXpb5jBNO3w",
        "colab_type": "text"
      },
      "source": [
        "Markov chain is a probabalistic model in which the probability of each event depends only on the state attained in the previous event. [markovify](https://github.com/jsvine/markovify) is a library for text generation by Markov chain.\n",
        "\n",
        "Use \"pip install markovify\" to install markovify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d89JFOANO3x",
        "colab_type": "code",
        "outputId": "3db2349b-0e60-4602-e2d6-c8f497a8f455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "!pip install markovify\n",
        "import markovify"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting markovify\n",
            "  Downloading https://files.pythonhosted.org/packages/de/c3/2e017f687e47e88eb9d8adf970527e2299fb566eba62112c2851ebb7ab93/markovify-0.8.0.tar.gz\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 8.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: markovify\n",
            "  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markovify: filename=markovify-0.8.0-cp36-none-any.whl size=10694 sha256=341c0e762308aa70c2a540543b39c65ba51b9d51ed612ce8f8f462b8b5d31eda\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/a8/92/35e2df870ff15a65657679dca105d190ec3c854a9f75435e40\n",
            "Successfully built markovify\n",
            "Installing collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.8.0 unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykeUNh6pFZHa",
        "colab_type": "code",
        "outputId": "355faa03-cd9d-43b7-a287-799777ddd304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "pip install wget"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6TT0amHFYMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wget\n",
        "url = 'https://raw.githubusercontent.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection/master/Sarcasm_Headlines_Dataset.json'\n",
        "filename = wget.download(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3dtkH9VNgf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parseJson(fname):\n",
        "    for line in open(fname, 'r+'):\n",
        "        yield eval(line)\n",
        "\n",
        "data = pd.DataFrame(list(parseJson(filename)))\n",
        "\n",
        "\n",
        "\n",
        "data.is_sarcastic = data.is_sarcastic.astype('bool')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9yfEfD-QNSX",
        "colab_type": "code",
        "outputId": "119c5fad-8c67-495b-9508-2c33f41b046c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>True</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>True</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>True</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   is_sarcastic  ...                                       article_link\n",
              "0          True  ...  https://www.theonion.com/thirtysomething-scien...\n",
              "1         False  ...  https://www.huffingtonpost.com/entry/donna-edw...\n",
              "2         False  ...  https://www.huffingtonpost.com/entry/eat-your-...\n",
              "3          True  ...  https://local.theonion.com/inclement-weather-p...\n",
              "4          True  ...  https://www.theonion.com/mother-comes-pretty-c...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UYA9GlERvkc",
        "colab_type": "code",
        "outputId": "ed4faf1a-bcdf-470e-fe76-10c0e0461d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len([headline for i,headline in enumerate(data.headline) if data.is_sarcastic[i]==True])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13634"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yolq4AkYNO30",
        "colab_type": "text"
      },
      "source": [
        "Download [Dataset.csv](https://drive.google.com/file/d/1raxIkJZ4lMTvgTB8eYjxu4Ux8aNL-x0s/view?usp=sharing) composed of sarcastic and serious headlines for the news. The csv-file consists of two columns. \"headline\" column contains texts of headlines. \"is_sarcastic\" column contain 0 if the hiadline is serious and 1 otherwise.\n",
        "\n",
        "Read dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_vDl5b8NO31",
        "colab_type": "code",
        "outputId": "1d0bc9ef-d029-4323-93c8-dbb0aaa307fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "texts_serious = [headline for i,headline in enumerate(data.headline) if data.is_sarcastic[i]==False] # list of serious hiadline texts\n",
        "texts_sarcastic = [headline for i,headline in enumerate(data.headline) if data.is_sarcastic[i]==True] # list of sarcastic hiadline texts\n",
        "# with open('Dataset.csv', encoding='utf-8') as f:\n",
        "#     # Rread csv-file by DictReader from csv library\n",
        "#     reader = csv.DictReader(f) \n",
        "#     for line in reader:\n",
        "#         # read texts of headline\n",
        "#         headline = line['headline'].strip()\n",
        "#         # read sarcasticity of headline\n",
        "#         is_sarcastic = int(line['is_sarcastic'].strip())\n",
        "#         if is_sarcastic:\n",
        "#             texts_sarcastic.append(headline)\n",
        "#         else:\n",
        "#             texts_serious.append(headline)\n",
        "print('Found {} sarcastic texts in Dataset'.format(data.groupby('is_sarcastic').size()[1]))\n",
        "print('Found {} serious texts in Dataset'.format(data.groupby('is_sarcastic').size()[0]))\n"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 13634 sarcastic texts in Dataset\n",
            "Found 14985 serious texts in Dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhtJNgwuNO35",
        "colab_type": "text"
      },
      "source": [
        "Merge headlines into one text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI9k6oOaNO35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_0 = '\\n '.join(texts_serious)\n",
        "text_1 = '\\n '.join(texts_sarcastic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YtvMBGwImVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import hmm_model as hmm\n",
        "N = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAxjkvEBIqvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_0 = text_0.split(\"\\n\")\n",
        "sentences_0 = [words.strip().split(\" \") for words in sentences_0]\n",
        "\n",
        "#As we are dealing with n-grams, sentences need to larger than n for it to work. \n",
        "sentences_0 = [words for words in sentences_0 if len(words)>=N]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tl9aREYI28B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "b9dbb6cd-fd55-443a-a5a5-b8a3c7397091"
      },
      "source": [
        "model_ser = hmm.HMM_MODEL(3)\n",
        "model_ser.create_model(sentences_0)\n",
        "model_ser.generate(15)"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['these simple keyboard shortcuts',\n",
              " 'hillary clinton in this hilarious acceptance speech',\n",
              " 'how do i need to know about mike kelley',\n",
              " \"gop congressman calls for 'world without nuclear weapons' on hiroshima bombing anniversary\",\n",
              " 'watch tom hardy sing (yes, sing) about a sense of humor is so important to so many boomers are redefining healthy aging tips for reading',\n",
              " '6 ways to cook eggs to reduce plastic waste this summer -- according to dermatologists',\n",
              " 'this week',\n",
              " 'the secret of ugly sweater day',\n",
              " 'how you play frisbee, but we all fell in love only works for everybody',\n",
              " 'how do i feel so guilty?',\n",
              " '10 worst states for business success',\n",
              " 'what it means to be',\n",
              " \"how to break guinness' 100-meter record\",\n",
              " 'trump ally roger stone says his time',\n",
              " 'an incomplete list of epic destinations for anyone who lives to travel']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0kozUmkJdZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_1 = text_1.split(\"\\n\")\n",
        "sentences_1 = [words.strip().split(\" \") for words in sentences_1]\n",
        "\n",
        "#As we are dealing with n-grams, sentences need to larger than n for it to work. \n",
        "sentences_0 = [words for words in sentences_1 if len(words)>=N]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKHxkAs5Jh4Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "e2977c9e-9891-4487-bb83-5afdf65e7234"
      },
      "source": [
        "model_sarc = hmm.HMM_MODEL(3)\n",
        "model_sarc.create_model(sentences_0)\n",
        "model_sarc.generate(15)"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['obama begins state of the park with close friends and shirtless stranger hanging around same website all day',\n",
              " 'woman has few backup scapegoats ready to put problem senators up front',\n",
              " 'man has never been washed he can pull off daring one-hour lunch break',\n",
              " 'man going to take short mental breakdowns for every woman he fucks',\n",
              " \"area woman's creativity\",\n",
              " 'new study finds link between education, smartness',\n",
              " \"man with 20 rifles can't remember which direction to rotate head back\",\n",
              " 'man always showing everyone his old things',\n",
              " 'area grandparents still have no clear goal',\n",
              " 'area bastards pick wrong guy to mess with this one every time',\n",
              " 'man with no discernible effect on date',\n",
              " 'new york',\n",
              " 'man does most of heavy lifting in sandwich',\n",
              " 'paul ryan announces new spearmint after-dinner big mac',\n",
              " \"nation's sleep experts recommend telling woman you would die for her\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XextATaPUinJ",
        "colab_type": "code",
        "outputId": "0d7d1da5-4691-4cfa-9887-ccf17e7353f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "markovify.Text(texts_sarcastic)"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<markovify.text.Text at 0x7f8999a76cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgh0nuOHNO38",
        "colab_type": "text"
      },
      "source": [
        "Create Markov chain model for serious headlines generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFez6CN-NO38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "serious_model = markovify.Text(texts_serious, state_size=2)\n",
        "sarcastic_model = markovify.Text(texts_sarcastic)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zZMwvZFNO3_",
        "colab_type": "text"
      },
      "source": [
        "Generate 20 serious headlines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDKt79ZcNO3_",
        "colab_type": "code",
        "outputId": "437684e9-81c5-4d13-a7ca-fb6d75532881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "for i in range(20):\n",
        "    print(serious_model.make_sentence())"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "study determines whether family or friends are annoying about their place in the arm to wearable tech\n",
            "lyft and uber pull out his daughter's cancer treatment\n",
            "silence on black death is a dairy farmer with no intel experience the house begin their debt limit increase\n",
            "can you tell if your goal weight\n",
            "what's in a single high school diploma at age 49\n",
            "syrian rebels with u.s. intelligence community\n",
            "how to emotionally recover from unicorn hair\n",
            "the average nfl career lasts just 3 minutes of breathing\n",
            "u.s. judge dismisses domestic violence\n",
            "trump nominee wants to change his own experience\n",
            "inspire the world notices\n",
            "5 missing after army helicopter crashes; crew captured by rebels\n",
            "rachel mcadams doesn't look real\n",
            "protesters stage third day of demonstrations in st. louis cardinals fans have a miscarriage\n",
            "no man in st. louis churches\n",
            "state rep. claims god is talking about this republican obamacare repeal without a woman can't be sexist because he was a catastrophe\n",
            "this adult take on a historic site?\n",
            "the 20 funniest tweets from women this week in...:the confusing and controversial tpp\n",
            "the spirit of paris fashion week\n",
            "5 steps to help you find the nerve to take on trump's business conflicts, russia ties\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDQHlJiFNO4B",
        "colab_type": "text"
      },
      "source": [
        "Create model and generate 20 sarcastic headlines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQOc0cWHNO4C",
        "colab_type": "code",
        "outputId": "32873ee3-ef3d-481f-8f01-360f6197758a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "source": [
        "for i in range(20):\n",
        "    print(sarcastic_model.make_sentence())"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "presidential debate sidetracked by booker, de blasio pac spends $30 million on amazon in one duty of providing good gynecologist recommendation\n",
            "area man thinking about getting into radiohead's kid a\n",
            "sweating, shaking man never in mood to do things he forced to look straight ahead\n",
            "general mills releases new lucky charms no longer going to pull big lever\n",
            "high school bully not so keen on president's new bangs\n",
            "local man helped every day of interviews with mainstream news outlets\n",
            "first report on neighborhood vibes\n",
            "man trying to solve elaborate riddle\n",
            "onion social ceo caught by law enforcement questions why accusers didn't come forward to carousel ride\n",
            "kfc blames popeyes for releasing serial rapist from prison in new jersey supreme court rules meryl streep unable to explain how pc culture destroying america\n",
            "trump demands william barr declares mueller investigation nearly done with most gruesome description of heaven during near-death experience specifically mentions second amendment again\n",
            "researchers find human vocal cords removed to prevent outbursts during debate\n",
            "cnn holds morning meeting to determine how americans were able to barely scrape by livelihood on own schedule\n",
            "magazine says you have won!\n",
            "disastrous ad campaign appeals to christ almighty\n",
            "paul ryan mentally logs 4,613th missed opportunity to explain what the fuck up about time to throw few more minutes\n",
            "townsperson in online rpg universe figures shield, gold pieces should be fully left up to discover favorite movie directed by woman\n",
            "financial experts recommend cutting down on abortions by outlawing all medical procedures couple picked out\n",
            "anguished, screaming trump bans father's ghost from press room for onion social users survive beta testing\n",
            "oscars attendees cower in awe as disembodied, all-knowing voice proclaims information about curbing endless cycle of government\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ5Jwl8TNO4E",
        "colab_type": "text"
      },
      "source": [
        "## Text generation by Variation Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K63RFOc9NO4E",
        "colab_type": "text"
      },
      "source": [
        "This part of tutorial based on [Text generation with a Variational Autoencoder](https://nicgian.github.io/text-generation-vae/) article. For sentence generation we use Variational Autoencoder (VAE) neural network model that is an extension seq2seq model. Originally VAE was described in [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf) paper. The idea behind Variational Autoencoder is that we impose predefined disribution (e.g., normal distribution) on the latent state formed by encoder. On the one hand this restriction alow us to sample random vectors from normal distribution and generate arbitrary sentences. On the othe hand this restriction form very dense well differentiated space without holes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6gGt8yRNO4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "from keras.layers import Bidirectional, Dense, Embedding, \\\n",
        "Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "from scipy import spatial\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import codecs\n",
        "import random\n",
        "import csv\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vppT5s81r3T-",
        "colab_type": "code",
        "outputId": "fd3acc8c-7fd5-4cb0-bae4-de1b967f97b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 406,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG9qeYThNO4H",
        "colab_type": "text"
      },
      "source": [
        "Dowload [GloVe](http://nlp.stanford.edu/data/glove.6B.zip) pretrained word embedding vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wliKwdPug6qN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bkWe_Rnl-Gv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRO5eyfjNO4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 15 # Max text length in tokens\n",
        "MAX_NB_WORDS = 20000 # Max words in dictionary\n",
        "EMBEDDING_DIM = 50 # Dimensionality of GloVe vectors \n",
        "\n",
        "GLOVE_EMBEDDING = 'glove.6B.50d.txt'\n",
        "\n",
        "train_data, val_data = train_test_split(data, test_size = .25, random_state= 42)\n",
        "train_data.to_csv('train_data.csv')\n",
        "val_data.to_csv('val_data.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZIhktGyNO4J",
        "colab_type": "text"
      },
      "source": [
        "Create sentence tokenizer and two dictionaries: word_to_id and id_to_word\n",
        "\n",
        "For tokenisation we use [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) from keras library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIqnRARPgCiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "txt = texts_sarcastic+ texts_serious\n",
        "txt = [re.sub(f\"\\'\",'',sent) for sent in txt]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUan2IgCNO4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #your code here\n",
        "\n",
        "tokenizer = Tokenizer(MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts_sarcastic)\n",
        "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= MAX_NB_WORDS}\n",
        "tokenizer.index_word = {e:i for e,i in tokenizer.index_word.items() if e <= MAX_NB_WORDS}\n",
        "word_to_id = tokenizer.word_index \n",
        "\n",
        "id_to_word = tokenizer.index_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGO1n0wyNO4L",
        "colab_type": "text"
      },
      "source": [
        "Tokenize sarcastic texts and create tensor composed of tokens indexes. If a sentence shorter than MAX_SEQUENCE_LENGTH we pad it. If a sentence longer than MAX_SEQUENCE_LENGTH we cut it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jsUsnDoNO4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#your code here\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts_sarcastic)\n",
        "sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
        "\n",
        "NB_WORDS = (min(tokenizer.num_words, len(word_to_id)) + 1 )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZbZ5hKllVpH",
        "colab_type": "code",
        "outputId": "bc66a458-eb5d-46b4-98f6-455c81c1c663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "train_data"
      ],
      "execution_count": 413,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28580</th>\n",
              "      <td>False</td>\n",
              "      <td>pit bull lovers gather in washington</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/pit-bull-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14354</th>\n",
              "      <td>True</td>\n",
              "      <td>new tech-support caste arises in india</td>\n",
              "      <td>https://www.theonion.com/new-tech-support-cast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19070</th>\n",
              "      <td>True</td>\n",
              "      <td>amazing original thing to become hated cliché ...</td>\n",
              "      <td>https://www.theonion.com/amazing-original-thin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5045</th>\n",
              "      <td>False</td>\n",
              "      <td>united airlines temporarily suspends cargo tra...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/united-su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24640</th>\n",
              "      <td>False</td>\n",
              "      <td>cuba and the united states: the long view</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/cuba-and-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21575</th>\n",
              "      <td>True</td>\n",
              "      <td>turnout lower than expected for gala central a...</td>\n",
              "      <td>https://www.theonion.com/turnout-lower-than-ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>True</td>\n",
              "      <td>retreating clinton campaign torches iowa town ...</td>\n",
              "      <td>https://politics.theonion.com/retreating-clint...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>True</td>\n",
              "      <td>national weather service to give hurricanes fu...</td>\n",
              "      <td>https://www.theonion.com/national-weather-serv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15795</th>\n",
              "      <td>True</td>\n",
              "      <td>christ returns for some of his old things</td>\n",
              "      <td>https://www.theonion.com/christ-returns-for-so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23654</th>\n",
              "      <td>True</td>\n",
              "      <td>loophole in curse lets archaeologist off the hook</td>\n",
              "      <td>https://www.theonion.com/loophole-in-curse-let...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>21464 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       is_sarcastic  ...                                       article_link\n",
              "28580         False  ...  https://www.huffingtonpost.com/entry/pit-bull-...\n",
              "14354          True  ...  https://www.theonion.com/new-tech-support-cast...\n",
              "19070          True  ...  https://www.theonion.com/amazing-original-thin...\n",
              "5045          False  ...  https://www.huffingtonpost.com/entry/united-su...\n",
              "24640         False  ...  https://www.huffingtonpost.com/entry/cuba-and-...\n",
              "...             ...  ...                                                ...\n",
              "21575          True  ...  https://www.theonion.com/turnout-lower-than-ex...\n",
              "5390           True  ...  https://politics.theonion.com/retreating-clint...\n",
              "860            True  ...  https://www.theonion.com/national-weather-serv...\n",
              "15795          True  ...  https://www.theonion.com/christ-returns-for-so...\n",
              "23654          True  ...  https://www.theonion.com/loophole-in-curse-let...\n",
              "\n",
              "[21464 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 413
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehtDdjiDNO4N",
        "colab_type": "text"
      },
      "source": [
        "Define batch generator to train a neural network\n",
        "\n",
        "For padding sentences to max length we use [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTj-u-QvNO4O",
        "colab_type": "code",
        "outputId": "4da9eb6e-b3ef-4b64-90f5-35420c6fa6ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "source": [
        "def sent_generator(TRAIN_DATA_FILE, batchsize):\n",
        "    # Create iterator that reads dataset file batch by batch \n",
        "    #your code here\n",
        "    seq = []\n",
        "    \n",
        "    #Shuffle the data file\n",
        "    #indexes = np.random.choice (range(len(TRAIN_DATA_FILE)),len(TRAIN_DATA_FILE),False)\n",
        "    \n",
        "    #shuffled = TRAIN_DATA_FILE.iloc[indexes]\n",
        "    \n",
        "    reader = [TRAIN_DATA_FILE.iloc[batchsize*i: batchsize*i +batchsize,:] for i in range(int(np.ceil(len(TRAIN_DATA_FILE)/batchsize)))]\n",
        "\n",
        "    \n",
        "    for df in reader:\n",
        "\n",
        "        # Read a column that contains headlines\n",
        "        #your code here\n",
        "        texts = list(df['headline'])\n",
        "        texts = [re.sub(\"\\'\",'',sent) for sent in texts]\n",
        "        if len (texts)<batchsize:\n",
        "          texts = texts + [' ']*(batchsize - len(texts))\n",
        "\n",
        "        # Tokenize texts and create padded tensor composed of tokens indexes\n",
        "        sequences = tokenizer.texts_to_sequences(texts)\n",
        "        sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "        \n",
        "        seq.append(tuple([sequences,sequences]))\n",
        "        # Return input-target pairs\n",
        "        #your code here\n",
        "\n",
        "        yield seq[0] \n",
        "\n",
        "print([a for a in sent_generator(train_data,  8)][:1][0])"
      ],
      "execution_count": 414,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([[ 2956,  3911,  5541,  2010,     3,   910,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0],\n",
            "       [    8,  3855,   567,  5712, 15779,     3,  2562,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0],\n",
            "       [ 1965,  1498,   197,     1,  1042,  8249, 10241,     3,   335,\n",
            "          433,     0,     0,     0,     0,     0],\n",
            "       [ 1116,  1906,  5460,  8562, 11333,  2108,     4,  5961,     0,\n",
            "            0,     0,     0,     0,     0,     0],\n",
            "       [ 8149,    33,    10,  1116,  1069,    10,   173,  2092,     0,\n",
            "            0,     0,     0,     0,     0,     0],\n",
            "       [ 1230,   229,    54,   279,    30,   103,    98,    87,   798,\n",
            "           40,   736,     0,     0,     0,     0],\n",
            "       [ 2270,  3832,  1567,  1465, 18332,   283,     1,  4445,    55,\n",
            "         1346,     0,     0,     0,     0,     0],\n",
            "       [ 2384,  4996,  1914,   151,  6945,   981,    89,  1271,     0,\n",
            "            0,     0,     0,     0,     0,     0]], dtype=int32), array([[ 2956,  3911,  5541,  2010,     3,   910,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0],\n",
            "       [    8,  3855,   567,  5712, 15779,     3,  2562,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0],\n",
            "       [ 1965,  1498,   197,     1,  1042,  8249, 10241,     3,   335,\n",
            "          433,     0,     0,     0,     0,     0],\n",
            "       [ 1116,  1906,  5460,  8562, 11333,  2108,     4,  5961,     0,\n",
            "            0,     0,     0,     0,     0,     0],\n",
            "       [ 8149,    33,    10,  1116,  1069,    10,   173,  2092,     0,\n",
            "            0,     0,     0,     0,     0,     0],\n",
            "       [ 1230,   229,    54,   279,    30,   103,    98,    87,   798,\n",
            "           40,   736,     0,     0,     0,     0],\n",
            "       [ 2270,  3832,  1567,  1465, 18332,   283,     1,  4445,    55,\n",
            "         1346,     0,     0,     0,     0,     0],\n",
            "       [ 2384,  4996,  1914,   151,  6945,   981,    89,  1271,     0,\n",
            "            0,     0,     0,     0,     0,     0]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRHwmKhGnFkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_generator(TRAIN_DATA_FILE, batchsize):\n",
        "    while 1:\n",
        "      sent_full_lists = []\n",
        "    # Create iterator that reads dataset file batch by batch \n",
        "    #your code here\n",
        "      iterations = TRAIN_DATA_FILE.shape[0] // batch_size\n",
        "      data = TRAIN_DATA_FILE\n",
        "        #TRAIN_DATA_FILE\n",
        "      for i in range(iterations):\n",
        "          df = data.iloc[i*batchsize:(i+1)*batchsize]\n",
        "        # Read a column that contains headlines      \n",
        "        # Tokenize texts and create padded tensor composed of tokens indexes\n",
        "          sent_list = list(df.headline)\n",
        "        #print(sent_list[0],sent_list[1])\n",
        "          sent_list=  tokenizer.texts_to_sequences(sent_list) \n",
        "          sent_list = pad_sequences(sent_list , maxlen = MAX_SEQUENCE_LENGTH, padding= 'post' , truncating= 'post' )\n",
        "        # Return input-target pairs\n",
        "          if i != iterations :\n",
        "                sent_full_lists.append(tuple([sent_list,sent_list]))\n",
        "        #your code here\n",
        "          \n",
        "          yield sent_full_lists[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ez-r3kaNO4Q",
        "colab_type": "text"
      },
      "source": [
        "Load pretrained GloVe vectors described in [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN0hxN0wNO4Q",
        "colab_type": "code",
        "outputId": "188cda08-1137-4aed-d49d-9544628529c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "embeddings_index = {}\n",
        "with open('glove.6B.50d.txt', encoding='utf-8') as f:\n",
        "    # read rows from file line by line\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0] # Get word\n",
        "        coefs = np.asarray(values[1:], dtype='float32') # Get elements of word's vector\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 416,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUDA9KQzNO4T",
        "colab_type": "text"
      },
      "source": [
        "Create matrix from embedding vectors. Any row of the matrix is a word's vector. We get words from the dictionary word_to_id defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkr2nZYUNO4U",
        "colab_type": "code",
        "outputId": "3eff6935-c234-43c0-84ba-bebd7b644c41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM)) # Create empty matrix (max number of tokens, dimension of the embedding vectors)\n",
        "not_in_glove=0\n",
        "nig = []\n",
        "for word, i in word_to_id.items():\n",
        "    #your code here\n",
        "    try:\n",
        "      glove_embedding_matrix[i] = embeddings_index[word]\n",
        "    except:\n",
        "      nig.append(word)\n",
        "      glove_embedding_matrix[i] = embeddings_index['unk']\n",
        "      not_in_glove+=1\n",
        "      continue\n",
        "# compute number of words which there aren't in the GloVe vectors\n",
        "print(f'Null word embeddings: {not_in_glove}')"
      ],
      "execution_count": 417,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Null word embeddings: 2388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ1gaqKSNO4X",
        "colab_type": "text"
      },
      "source": [
        "Define parameters of the net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4rw7A3mNO4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "max_len = MAX_SEQUENCE_LENGTH\n",
        "emb_dim = EMBEDDING_DIM\n",
        "latent_dim = 32 # dimensionality of the hidden state in encoder and decoder RNN's\n",
        "intermediate_dim = 96 # dimensionality of variational space into which we map encoder's hidden state\n",
        "epsilon_std = 1.0 # standard deviation of gaussian noise\n",
        "act = ELU() # activation function of projection layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sarNC5v1NO4c",
        "colab_type": "text"
      },
      "source": [
        "Encoder of the variational autoencoder. It based on bidirectional LSTM\n",
        "\n",
        "We use following layers: [Input](https://www.tensorflow.org/api_docs/python/tf/keras/Input), [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding), [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional), [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout), [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), [ELU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ELU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgRQeNiGNO4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Input(batch_shape=(None, max_len)) # Input layer fo the net. \n",
        "# Write an embedding layer for the input sequences of indexes. \n",
        "# Use pretrained word embeddings as a embedding layer weights and don't update these weights\n",
        "\n",
        "#your code here\n",
        "x_embed = Embedding(NB_WORDS, emb_dim, weights=[glove_embedding_matrix],\n",
        "                            input_length=max_len, trainable=False)(x)\n",
        "\n",
        "\n",
        "# Bidirectional LSTM encoder\n",
        "\n",
        "#your code here\n",
        "h = Bidirectional(LSTM(latent_dim, return_sequences=False, recurrent_dropout=0.2))(x_embed)\n",
        "\n",
        "\n",
        "\n",
        "h = Dropout(0.2)(h) \n",
        "# Dropout for the BiLSTM layer to avoid overfitting \n",
        "\n",
        "# Fully-connected layer to map encoder hidden state into variational space\n",
        "\n",
        "#your code here\n",
        "h = Dense(intermediate_dim, activation='linear')(h)\n",
        "h = act(h)\n",
        "\n",
        "h = Dropout(0.2)(h) # Dropout for the fully-connected layer to avoid overfitting \n",
        "z_mean = Dense(latent_dim)(h) # Fully-connected layer to map variational space into means space \n",
        "z_log_var = Dense(latent_dim)(h) # Fully-connected layer to map variational space into standard deviations space "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61_EJbujNO4f",
        "colab_type": "text"
      },
      "source": [
        "The mechanism for sampling hidden vectors from variational space\n",
        "\n",
        "To apply it to our model we use [Lambda](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda) layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5OGmTyuNO4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sampling(args):\n",
        "    # Vectors from means space and standard deviations space respectively\n",
        "    z_mean, z_log_var = args\n",
        "    # Sample random vectors from normal distribution with mean=0 and std=epsilon_std\n",
        "    #epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
        "    #                          stddev=epsilon_std)\n",
        "    epsilon = np.random.normal(loc = 0 , scale= epsilon_std , size = z_mean.shape[0])\n",
        "\n",
        "    #your code here\n",
        "    \n",
        "    # Get new hidden state for decoder using vectors from means, standard deviations and normal random spaces\n",
        "    #return z_mean + K.exp(z_log_var/2) * epsilon\n",
        "    return z_mean + z_log_var * epsilon\n",
        "\n",
        "# Get hidden states for the decoder\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U89BoV9NO4h",
        "colab_type": "text"
      },
      "source": [
        "Define decoder of the autoencoder\n",
        "\n",
        "For this we use following layers: [RepeatVector](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RepeatVector), [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [TimeDistributed](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed), [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kD2_yfJNO4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Repeat the hidden state vector to form input sequence for decoder\n",
        "repeated_context = RepeatVector(max_len)\n",
        "# Decoder LSTM\n",
        "decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n",
        "# Layer for mapping from hidden satates space to the space of dimension equal to size of vocabulary\n",
        "decoder_mean = TimeDistributed(Dense(NB_WORDS, activation='linear'))\n",
        "# Generated sequence\n",
        "h_decoded = decoder_h(repeated_context(z))\n",
        "# Decode every time step vector of the decoded sequence into space of dimension equal to size of vocabulary\n",
        "x_decoded_mean = decoder_mean(h_decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f90V8-ENO4i",
        "colab_type": "text"
      },
      "source": [
        "Define layer for loss computing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIXaSO8NO4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zero_loss(y_true, y_pred):\n",
        "    # Return tensor filled with ones with shape equal generated sequence shape\n",
        "    return K.zeros_like(y_pred)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU1cXC0tNO4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomVariationalLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.is_placeholder = True\n",
        "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
        "        # Create tensor (batch_size, max_sequence_len) filled with ones to consider all elements of generated sequence \n",
        "        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n",
        "\n",
        "    def vae_loss(self, x, x_decoded_mean):\n",
        "        # Get tensor with similar shape as x\n",
        "        labels = tf.cast(x, tf.int32)\n",
        "        # Compute sequence reconstruction loss\n",
        "        xent_loss = K.sum(tfa.seq2seq.sequence_loss(x_decoded_mean, labels,\n",
        "                                                    weights=self.target_weights,\n",
        "                                                    average_across_timesteps=False,\n",
        "                                                    average_across_batch=False), axis=-1)\n",
        "        # Compute KL-divergence as Variational loss \n",
        "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        # Composite loss (reconstruction loss + Variational loss)\n",
        "        return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs[0] # input sequence\n",
        "        x_decoded_mean = inputs[1] # reconstructed sequence\n",
        "        print(x.shape, x_decoded_mean.shape)\n",
        "        loss = self.vae_loss(x, x_decoded_mean) # Compute loss of the model\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        # we don't use this output, but it has to have the correct shape\n",
        "        return K.ones_like(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8GYYLM0NO4m",
        "colab_type": "text"
      },
      "source": [
        "Assemble the model\n",
        "\n",
        "To define model we use [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) from tensorflow.\n",
        "\n",
        "To train model employ [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimization algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ9Zwt9DNO4n",
        "colab_type": "code",
        "outputId": "9714c3a1-5398-4659-a3b4-c795c99bbb8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "# Create custom layer for loss computing\n",
        "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
        "\n",
        "vae = Model(x, [loss_layer])\n",
        "# Use Adam optimizer with learning rate = 0.01\n",
        "opt = Adam(lr=0.01)\n",
        "vae.compile(optimizer=opt, loss=[zero_loss])\n",
        "# Show model structure\n",
        "vae.summary()"
      ],
      "execution_count": 424,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 15) (None, 15, 20001)\n",
            "Model: \"model_31\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           (None, 15)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_18 (Embedding)        (None, 15, 50)       1000050     input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_18 (Bidirectional (None, 64)           21248       embedding_18[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 64)           0           bidirectional_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_67 (Dense)                (None, 96)           6240        dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "elu_17 (ELU)                    (None, 96)           0           dense_67[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 96)           0           elu_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_68 (Dense)                (None, 32)           3104        dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_69 (Dense)                (None, 32)           3104        dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (None, 32)           0           dense_68[0][0]                   \n",
            "                                                                 dense_69[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_16 (RepeatVector) (None, 15, 32)       0           lambda_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_34 (LSTM)                  (None, 15, 96)       49536       repeat_vector_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_16 (TimeDistri (None, 15, 20001)    1940097     lstm_34[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "custom_variational_layer_17 (Cu [(None, 15), (None,  0           input_26[0][0]                   \n",
            "                                                                 time_distributed_16[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 3,023,379\n",
            "Trainable params: 2,023,329\n",
            "Non-trainable params: 1,000,050\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUq-wpywNO4r",
        "colab_type": "text"
      },
      "source": [
        "Checkpoint function to save states of our model during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N_2MDF_NO4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_checkpoint(model_name):\n",
        "    filepath = f\"models/{model_name}.h5\"\n",
        "    directory = os.path.dirname(filepath)\n",
        "    try:\n",
        "        # Check if directory exists\n",
        "        os.stat(directory)\n",
        "    except:\n",
        "        # If directory doesn't exist, create the directory\n",
        "        os.mkdir(directory)\n",
        "    # Save model states\n",
        "    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=False)\n",
        "    return checkpointer\n",
        "\n",
        "# Create model checkpointer\n",
        "checkpointer = create_model_checkpoint('vae_seq2seq')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Cw3RxnCNO4u",
        "colab_type": "text"
      },
      "source": [
        "Train model, test model after each apoch and save model's state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRTTpZOU5Zcf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "a61515a3-d6ab-41dd-9f9f-8f9b11710fe3"
      },
      "source": [
        "sents = val_data['headline']\n",
        "sents = [re.sub(r\"\\'\",'',sent) for sent in sents]\n",
        "val_1_data = tokenizer.texts_to_sequences(sents)\n",
        "val_1_data = pad_sequences(val_1_data, maxlen=MAX_SEQUENCE_LENGTH,padding = 'post')\n",
        "val_1_data = val_1_data[:np.max([i for i in range(val_1_data.shape[0]) if i % 128 == 0]),:]\n",
        "val_1_data"
      ],
      "execution_count": 426,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1069, 1900,    1, ...,    0,    0,    0],\n",
              "       [1027, 1506, 2720, ...,    0,    0,    0],\n",
              "       [   2,   78,  100, ..., 3343,  893, 2349],\n",
              "       ...,\n",
              "       [ 454,   50,  632, ...,    0,    0,    0],\n",
              "       [2019, 1071,  501, ...,    0,    0,    0],\n",
              "       [7671, 1178,  173, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 426
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iscsBzrwNO4v",
        "colab_type": "code",
        "outputId": "ebe4e009-0f65-4f55-cced-39f4af143911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nb_epoch = 20 # number of epochs for model training\n",
        "n_steps = train_data.shape[0] // batch_size # Number of steps per epoch\n",
        "for counter in range(nb_epoch):\n",
        "    print('-------epoch: ', counter, '-------')\n",
        "    # Train model. Test and save model after every epoch\n",
        "    vae.fit_generator(sent_generator(train_data, batch_size),\n",
        "                      steps_per_epoch=n_steps, epochs=1, callbacks=[checkpointer],\n",
        "                      )#validation_data=(val_1_data,val_1_data))\n",
        "vae.save(r'vae_lstmFull32dim96hid.h5')"
      ],
      "execution_count": 427,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------epoch:  0 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 29s 172ms/step - loss: 50.0156\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  1 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 167ms/step - loss: 14.7629\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  2 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 168ms/step - loss: 9.9880\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  3 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 168ms/step - loss: 5.7387\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  4 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 169ms/step - loss: 4.4322\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  5 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 165ms/step - loss: 3.4077\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  6 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 166ms/step - loss: 3.0174\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  7 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 168ms/step - loss: 2.4183\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  8 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 168ms/step - loss: 6.2209\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  9 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 169ms/step - loss: 2.1558\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  10 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 169ms/step - loss: 1.6702\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  11 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 168ms/step - loss: 1.5324\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  12 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 167ms/step - loss: 1.3112\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  13 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 168ms/step - loss: 1.4391\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  14 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 167ms/step - loss: 1.3354\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  15 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 168ms/step - loss: 1.4006\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  16 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 165ms/step - loss: 1.5557\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  17 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 169ms/step - loss: 0.9577\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  18 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 166ms/step - loss: 1.0620\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n",
            "-------epoch:  19 -------\n",
            "Epoch 1/1\n",
            "167/167 [==============================] - 28s 166ms/step - loss: 1.2819\n",
            "\n",
            "Epoch 00001: saving model to models/vae_seq2seq.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjXY9bJMNO4x",
        "colab_type": "text"
      },
      "source": [
        "Assemble encoder and decoder for sentence generation sampled from variational space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HArdLjkyeNY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "vae.load_weights(\"/content/models/vae_seq2seq.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTJBfCHxNO4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make separate encoder to encode input sentence\n",
        "encoder = Model(x, z_mean)\n",
        "# Input layer for decoder to decode vectors sampled from variational space \n",
        "decoder_input = Input(shape=(latent_dim,))\n",
        "# Apply LSTM to decode hidden vector into sequence\n",
        "_h_decoded = decoder_h(repeated_context(decoder_input))\n",
        "# Decode every time step vector of the decoded sequence into space of dimension equal to size of vocabulary\n",
        "_x_decoded_mean = decoder_mean(_h_decoded)\n",
        "# Apply softmax to get most probable token\n",
        "_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\n",
        "# Make decoderfor sempled sentences\n",
        "generator = Model(decoder_input, _x_decoded_mean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4a0y7FvNO4z",
        "colab_type": "text"
      },
      "source": [
        "Generate sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3EVFD9U4Kj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_generator_test(TRAIN_DATA_FILE, batchsize):\n",
        "    sent_full_lists = []\n",
        "    # Create iterator that reads dataset file batch by batch \n",
        "    #your code here\n",
        "    data = pd.read_csv( TRAIN_DATA_FILE, usecols = ['headline'] , nrows= 500 )\n",
        "        #TRAIN_DATA_FILE\n",
        "    sent_list = list(data.headline)\n",
        "    sent_list = [re.sub(r\"\\'\",'',sent) for sent in sent_list]\n",
        "\n",
        "        #print(sent_list[0],sent_list[1])\n",
        "    sent_list=  tokenizer.texts_to_sequences(sent_list) \n",
        "    sent_list = pad_sequences(sent_list , maxlen = MAX_SEQUENCE_LENGTH, padding= 'post' , truncating= 'post' )\n",
        "        # Return input-target pairs\n",
        "    sent_full_lists.append(sent_list)\n",
        "        #your code here\n",
        "    return sent_full_lists[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUVjUl_L4LSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = sent_generator_test('val_data.csv',16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCwkXY6auPMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index2word = {v: k for k, v in word_to_id.items()}\n",
        "\n",
        "def makepredictions(val_data, batchsize, index2word, indexes=None):\n",
        "  if indexes is None:\n",
        "    indexes = np.random.choice(np.arange(val_data.shape[0]),batchsize)\n",
        "  for index in indexes:\n",
        "    sent_encoded = encoder.predict(np.expand_dims(val_data[index],axis=0))\n",
        "    x_test_reconstructed = generator.predict(sent_encoded)\n",
        "    reconstructed_indexes = np.apply_along_axis(np.argmax, -1, x_test_reconstructed)\n",
        "    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
        "    word_list = ' '.join([w for w in word_list[0] if w!= 'None'])\n",
        "    original_sent = list(np.vectorize(index2word.get)(val_1_data[index]))\n",
        "    original_sent = ' '.join([w for w in original_sent if w!='None'])\n",
        "    print(f\"Original sentence:\\t{original_sent}\")\n",
        "    print(f\"Reconstructed sentence:\\t{word_list}\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv6biErtuRyO",
        "colab_type": "code",
        "outputId": "29b110d6-468e-4cb0-fc8d-14991973b73b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "makepredictions(val_1_data,10,index2word)"
      ],
      "execution_count": 443,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sentence:\tonline activists unsure about offensiveness of article figure destroy authors life just in case\n",
            "Reconstructed sentence:\treport it's important of 27 every word of divorce document you sign\n",
            "\n",
            "Original sentence:\tasimo tricked into falling down stairs\n",
            "Reconstructed sentence:\twhen responds dead father's body at on months\n",
            "\n",
            "Original sentence:\tsanders could pull off michigan style upset in ohio\n",
            "Reconstructed sentence:\tteen responds to shout out at the the\n",
            "\n",
            "Original sentence:\tartists as global citizens\n",
            "Reconstructed sentence:\tman lived dead father's body for four months\n",
            "\n",
            "Original sentence:\tnationwide tax day demand donald trump release his tax returns\n",
            "Reconstructed sentence:\tman to americans support for travel on on\n",
            "\n",
            "Original sentence:\textra extra how to get your face on screen\n",
            "Reconstructed sentence:\tup to to robbers storm in in flash\n",
            "\n",
            "Original sentence:\t3 tips for improving the productivity of your sales team\n",
            "Reconstructed sentence:\tman to dead father's body for four months\n",
            "\n",
            "Original sentence:\tstress and performance anxiety part 2\n",
            "Reconstructed sentence:\tlove a a brief on moving on dependence\n",
            "\n",
            "Original sentence:\thillary clinton torches the lip service of ivanka trump\n",
            "Reconstructed sentence:\thow many americans support body travel ban months\n",
            "\n",
            "Original sentence:\ttrump blasts critics who judge neo nazi groups by most extreme members\n",
            "Reconstructed sentence:\tman parent wishes she had she of abandoning child first\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf7yrbD6NO4z",
        "colab_type": "code",
        "outputId": "c6a118dc-fe9e-46ca-ff37-3aff061375e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Dictionary maps indexes to words\n",
        "index2word = {v: k for k, v in word_to_id.items()}\n",
        "# Fit sentences from validation set into encoder\n",
        "sent_encoded = encoder.predict(test_data, batch_size=16)\n",
        "# Decode encoded sentences\n",
        "x_test_reconstructed = generator.predict(sent_encoded)\n",
        "\n",
        "sent_idx = np.random.choice(np.arange(test_data.shape[0]))\n",
        "# Get words indexes with highest probability for the 400th sentence from validation set\n",
        "reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[sent_idx])\n",
        "# Map indexes of generated sentence to words\n",
        "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
        "word_list = ' '.join([w for w in word_list if w!='None'])\n",
        "print(word_list)\n",
        "# Map indexes of input sentence to words\n",
        "original_sent = list(np.vectorize(index2word.get)(val_1_data[sent_idx]))\n",
        "original_sent = ' '.join([w for w in original_sent if w!='None'])\n",
        "print(original_sent)"
      ],
      "execution_count": 442,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "why it's important to read every word of every divorce document you sign\n",
            "mom wants to know if the people who live in your apartment building are nice\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmZ8GgGWEO3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}